{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GAEe7p2HF_I",
        "outputId": "0ef56636-843b-4f29-cd54-f88701f9c5aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==============------------------------------------] 29.8% 495.9/1662.8MB downloaded"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "word2VecModel = api.load(\"glove-twitter-25\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "filePath = \"simplePuzzles.json\"\n",
        "\n",
        "# If the json file is already fetched from our GCS, do not re-fetch.\n",
        "if not os.path.exists(filePath):\n",
        "  # You can upload the file to https://console.cloud.google.com/storage/browser/cs221team\n",
        "  # Accessible @ https://storage.googleapis.com/cs221team/simplePuzzles.json\n",
        "  !gsutil cp gs://cs221team/simplePuzzles.json simplePuzzles.json\n"
      ],
      "metadata": {
        "id": "_rzHUBjWHRqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def average_multi_word_to_vec(str, model):\n",
        "  \"\"\"\n",
        "  for a given word and word2vec model, return a vector representation. If none of the\n",
        "  sub words are found in word2vec model, return None.\n",
        "  \"\"\"\n",
        "  vecs = [np.array(model[c]) for c in str.lower().split() if c in model]\n",
        "  return np.array(np.mean(vecs, axis=0)) if len(vecs) != 0 else None\n",
        "\n",
        "def load_samples(filePath, model):\n",
        "  \"\"\"\n",
        "  load from the json game data and output train/test split. Each input is mapped to\n",
        "  a category. An input and category could contain multiple words and this function\n",
        "  averages the vector embedded values. The training set is an ndarray of size (num_samples, word2VecModel.vector_size)\n",
        "  The label set is an ndarray of the same dimension. For example:\n",
        "  x_train: [v1, v2, v3, v4, v5, v6, v7, v8]\n",
        "  y_train: [c1, c1, c1, c1, c2, c2, c2, c2]\n",
        "  where vn are different word vectors and cn are different category vectors. Since the game\n",
        "  always have four words mapped to the same category, we will see duplicate categories in y_train.\n",
        "  Note: if a word is not found in the word2vec model, it is ignored. If a category is not found in the\n",
        "  model, all associated words for it is ignored.\n",
        "\n",
        "  filePath: the path to game data in json format\n",
        "  model: a word2vec model\n",
        "  \"\"\"\n",
        "\n",
        "  with open(filePath) as f:\n",
        "    data = json.load(f)\n",
        "    samples = []\n",
        "    sample_labels = []\n",
        "\n",
        "    print(f\"processing {len(data)} entries of game data\")\n",
        "    for i in range(len(data)):\n",
        "      # Sample: {'id': 1, 'wordCategories': {'WET WEATHER': ['HAIL', 'RAIN', 'SLEET', 'SNOW'], 'NBA TEAMS': ['BUCKS', 'HEAT', 'JAZZ', 'NETS'], 'KEYBOARD KEYS': ['OPTION', 'RETURN', 'SHIFT', 'TAB'], 'PALINDROMES': ['KAYAK', 'LEVEL', 'MOM', 'RACECAR']}}\n",
        "\n",
        "      for category, input in data[i]['wordCategories'].items():\n",
        "        # if each category has multiple words, average the vector together. If all of the words\n",
        "        # are not in word2vec, skip this sample.\n",
        "        label = average_multi_word_to_vec(category, model)\n",
        "        if label is None:\n",
        "          continue\n",
        "\n",
        "        for words in input:\n",
        "          # it's possible the input can have multiple words. average them.\n",
        "          average_input_vector = average_multi_word_to_vec(words, model)\n",
        "          if average_input_vector is None:\n",
        "            continue\n",
        "\n",
        "          samples.append(average_input_vector)\n",
        "          sample_labels.append(label)\n",
        "\n",
        "\n",
        "    assert len(samples) == len(sample_labels)\n",
        "    assert samples[0].shape == sample_labels[0].shape\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(\n",
        "    np.array(samples), np.array(sample_labels), test_size=0.05, random_state=0)\n",
        "  print(f\"num of train samples {len(x_train)}, num of test samples {len(x_test)}\")\n",
        "  print(f\"x_train shape {x_train.shape} y_train shape {y_train.shape}\")\n",
        "  return x_train, y_train, x_test, y_test"
      ],
      "metadata": {
        "id": "G-texjXCH0Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class NeuralNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple network that predicts the output word vector\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(25, 100)\n",
        "        self.fc2 = nn.Linear(100, 75)\n",
        "        self.fc3 = nn.Linear(75, 25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "FizR038CbK5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "def train_network(model, X_train, Y_train, X_test, Y_test):\n",
        "  \"\"\"\n",
        "  Train a given network\n",
        "  \"\"\"\n",
        "  loss_fn = nn.MSELoss()  # mean square error\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "  n_epochs = 100   # number of epochs to run\n",
        "  batch_size = 100  # size of each batch\n",
        "  batch_start = torch.arange(0, len(X_train), batch_size)\n",
        "\n",
        "  # Hold the best model\n",
        "  best_mse = np.inf   # init to infinity\n",
        "  best_weights = None\n",
        "  history = []\n",
        "  model.float()\n",
        "  for epoch in range(n_epochs):\n",
        "      model.train()\n",
        "      with tqdm(batch_start, unit=\"batch\", mininterval=0, position=0, leave=True) as bar:\n",
        "          bar.set_description(f\"Epoch {epoch}\")\n",
        "          for start in bar:\n",
        "              X_batch = X_train[start:start+batch_size]\n",
        "              Y_batch = Y_train[start:start+batch_size]\n",
        "\n",
        "              Y_pred = model(X_batch)\n",
        "              loss = loss_fn(Y_pred, Y_batch)\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              # print progress\n",
        "              bar.set_postfix(mse=float(loss))\n",
        "\n",
        "      # evaluate accuracy at end of each epoch\n",
        "      model.eval()\n",
        "      Y_pred = model(X_test)\n",
        "      mse = loss_fn(Y_pred, Y_test)\n",
        "      mse = float(mse)\n",
        "      history.append(mse)\n",
        "      if mse < best_mse:\n",
        "          best_mse = mse\n",
        "          best_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  return history, best_mse, best_weights"
      ],
      "metadata": {
        "id": "GRf-3QFqeh52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train, X_test, Y_test = load_samples(filePath, word2VecModel)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
        "Y_train = torch.tensor(Y_train, dtype=torch.float32, device=device)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
        "Y_test = torch.tensor(Y_test, dtype=torch.float32, device=device)\n",
        "\n",
        "neural_network = NeuralNetwork().to(device)\n",
        "history, best_mse, best_weights = train_network(neural_network, X_train, Y_train, X_test, Y_test)"
      ],
      "metadata": {
        "id": "Vk8sbIfWfZS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(\"MSE: %.2f\" % best_mse)\n",
        "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
        "plt.plot(history)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wwCb8T8ohyi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(filePath, neural_network, word2vecModel, best_weights):\n",
        "  \"\"\"\n",
        "  evaluate our neural network\n",
        "  \"\"\"\n",
        "  loss = nn.MSELoss()\n",
        "  neural_network.load_state_dict(best_weights)\n",
        "  positive = 0\n",
        "  total_tests = 0\n",
        "\n",
        "  with open(filePath) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "    neural_network.eval()\n",
        "    with torch.no_grad():\n",
        "      for i in range(len(data)):\n",
        "        categories = list(data[i]['wordCategories'].keys())\n",
        "        average_category_vecs = [average_multi_word_to_vec(cat, word2vecModel) for cat in categories]\n",
        "        average_category_vecs = [torch.tensor(v, dtype=torch.float32, device=device) for v in average_category_vecs if v is not None]\n",
        "        # if any category cannot be mapped to vec by word2vec, skip it\n",
        "        if len(average_category_vecs) != 4:\n",
        "          continue\n",
        "        for cat in categories:\n",
        "          words = data[i]['wordCategories'][cat]\n",
        "          word_vecs = [average_multi_word_to_vec(word, word2vecModel) for word in words]\n",
        "          word_vecs = [torch.tensor(v, dtype=torch.float32, device=device) for v in word_vecs if v is not None]\n",
        "          for v in word_vecs:\n",
        "            output = neural_network(v)\n",
        "            loss_list = [float(loss(output, category_vec)) for category_vec in average_category_vecs]\n",
        "            loss_and_category = zip(loss_list, categories)\n",
        "\n",
        "            # sort by loss\n",
        "            result = sorted(zip(loss_list, categories), key=lambda x: x[0])\n",
        "            if result[0][1] == cat:\n",
        "              positive += 1\n",
        "            total_tests += 1\n",
        "  print(\"words matching true category:\", positive, \"out of total:\", total_tests, \"accuracy:\", positive / total_tests)\n",
        "\n",
        "eval_model(filePath, neural_network, word2VecModel, best_weights)"
      ],
      "metadata": {
        "id": "Q0c5bOYKgnUn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
